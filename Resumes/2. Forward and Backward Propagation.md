# Deep Learning in One Slide
---

A family of parametric, non-linear and hierarchical representation learning functions, which are massively optimized with stochastic gradient descent to encode domain knowledge, i.e., domain invariances, stationary. 

$$
a_L(x; \theta_{1, ..., L}) = h_L(h_{L-1}(...h_1(x; \theta_1); \theta_2); \theta_L))
$$
  
  where $a_L$ is the output of the last layer, $x$ is the input, $\theta_{1, ..., L}$ are the parameters, and $h_{1, ..., L}$ are the activation functions.
  Given training copus $X = \{x_1, ..., x_N\}, Y = \{y_1, ..., y_N\}$, the optimization problem is to find optimal parameters $\theta_{1, ..., L}$ by minimizing the empirical risk.

# Deep Feedforward Networks
---
Feedforward neural networks 
- Also called MLPs
- The goal is to approximate some function f
- A feedforward network defines a mapping:  y = f(x; θ) and learns the value of the parameters θ that result in the best function approximation
No feedback connections
- When including feedback connections, we obtain recurrent neural networks 
- Note: brains have mainly feedback connections 

A composite of functions 
We can simplify the notation: 
$$
a_L = f(x; \theta) = h_l \circ h_{L-1} \circ ... \circ h_1(x; \theta)
$$
where each $h_l$ is a non-linear activation function parameterized by $\theta_l$.


## Neural Networks as Blocks 
---

With the last notation, we can visualize networks as blocks 
  ![[Screenshot 2024-11-03 at 20.30.47.png]]
Module <=> Building Block <=> Transformation <=> Function 
A module receives as input either data x for another module's output 
A module returns an output based on its activation function $h$ 
A module may or may not have trainable parameters $w$ (example: max pool, exp(x))

# Requirements
---
1. Activations must be 1st order differentiable (almost) everywhere
2. Take special care when there are cycles in the architecture of blocks 

Most models are feedforward networks (e.g., CNNs, Transformers)

# One Slide on Recurrency
---
The module's past output is the module's future input 
We must take care of cycles, i.e., unfold the graph (Recurrent Networks)
Were completely out of fashion, but is a comeback possible with xLSTM? 

# Accepting a Non-Linear World
---
XOR can be solved with a linear model by transforming its input through the small neural network below. 

![[Screenshot 2024-11-03 at 20.37.41.png]]
# Training Goal and Overview
---

We have a dataset of inputs and outputs 
Initialize all weights and biases with random values. 

Learn weights and biases through "forward-backward" propagation.
1. Forward Step: Map input to the predicted output.
2. Loss step: Compare predicted output to ground truth output.
3. Backward step: correct predictions by propagating gradients.

# The Linear/Fully Connected Layer
---

![[Screenshot 2024-11-03 at 20.40.37.png]]

# Forward Propagation
---

When using linear layers, essentially repeated application of perceptrons:
1. Start from inputs, multiply with weights, sum, add bias 
2. Repeat for all following layers until you reach the end 

There is one main new element (next to the multiple layers):
- Activation functions after each layer. 

# Why have activation functions? 
---

Each hidden/output is a linear sum. 
A combination of linear functions is a linear function. 
$$
\begin{align*}
v(x) &= a x + b \\
w(x) &= c x + d \\
v(w(x)) &= a(c x + d) + b = ac x + ad + b
\end{align*}
$$

Activation functions transforms the outputs of each neuron.
This results in non-linear functions. 

# Activation Functions
---

Defines how the weighted sum of the input is transformed into an output in a layer of the network. 
If output range limited, then called a "squashing function".
The choice of activation function has a large impact on the capability and performance of the neural network.
Different Activation Functions may be combined, but rare. 
All hidden layers typically use the same activation function. 
Need to be differentiable at most points. 

## The Sigmoid Activation Function

![[Screenshot 2024-11-03 at 20.46.25.png]]

## The Tanh Activation Function

![[Screenshot 2024-11-03 at 20.47.47.png]]
## The Rectified Linear Activation Function (ReLU)

![[Screenshot 2024-11-03 at 20.48.11.png]]
## Advantages of ReLU 

Sparse Activation: In randomly initialized network, $~50\%$ of neurons are inactive. 
Better gradient propagation: Fewer vanishing gradient problems compared to sigmoidal activation functions that saturate in both directions. 
Computational efficiency: ReLU is computationally cheaper than sigmoid and tanh, only comparison, addition and multiplication 

## Limitations of ReLU

Non-Differentiable at zero; however, it is differentiable anywhere else, and the value of the derivative at zero can be arbitrarly chosen to be 0 or 1. 
Not zero centered.
Unbounded.
Dead Neuron Problem: neurons can sometimes be pushed into states in which they become inactive for essentially all inputs. Higher learning rates might help. 

# ReLU VS Sigmoid: Which one is more non-linear? 
---
ReLU is piecewise linear, while Sigmoid is a smooth curve. 
ReLU is more non-linear than Sigmoid. 

# Leaky ReLU
---

![[Screenshot 2024-11-03 at 20.52.32.png]]
# ELU
---
![[Screenshot 2024-11-03 at 20.53.27.png]]

# Activation Functions in One Image
---
![[Screenshot 2024-11-03 at 20.53.55.png]]

# How to Choose an Activation Function 
---
## Hidden Layers
- In modern neural networks, the default recommendation is to use ReLU or GELU
- (Recurrent Neural Networks: tanh or sigmoid)

## Output Layers

**Regression:** one node, linear activation 
**Binary Classification:** One node, sigmoid activation 
**Multiclass Classification:** one node per class, softmax activation 
**Multilabel Classification:** one node per class, sigmoid activation 


# Cost Functions
---

## Binary Classification


![[Screenshot 2024-11-03 at 20.56.51.png]]

Maximize: $p_i^{y_i} (1-p_i)^{1-y_i}$
Minimize: $- \log(p_i^{y_i} (1-p_i)^{1-y_i})$

## Multiclass Classification

Outputs probability distribution. Softmax 
![[Screenshot 2024-11-03 at 20.58.03.png]]

# Architecture Design 
---
The overall structure of the network: 
1. How many units should it should have 
2. How those units should be connected to each other 
Neural neworks are organized into groups of units, called layers in a chain structure. 

The first layer is given by: 
$$
h^{(1)} = g^{(1)}(W^{(1)T}x + b^{(1)})
$$
where $x$ is the input, $W$ is the weight matrix, $b$ is the bias, and $g$ is the activation function. 
The second layer is:
$$
h^{(2)} = g^{(2)}(W^{(2)T}h^{(1)} + b^{(2)})
$$
and so on. 

# Universal Approximation Theorem
---
Feedforward neural networks with a single hidden layer containing a finite number of neurons can approximate continuous functions on compact subsets of $\mathbb{R}^n$, under mild assumptions on the activation function. 
However, no guarantees are given about the number of neurons required and that the learning algorithm will be able to learn that function. May no tbe able to learn the function. Might choose the wrong function due to overfitiing. 

# Width and Depth
---
In the worse case, an exponential number of hidden units. 
A deep rectifir net can require an exponential number of hidden units with a shallow (one hidden layer) network

We like deep models in deep learning:
1. Can reduce the number of number of units required to represent the same function
2. Can reduce the amount of generalization error 
3. Deeper Networks often generalize better

# FFN: A Jungle of Architectures 
---
Perceptron, MLPs
RNNs, LSTMs, GRUs
Vanilla, Variational, Denoising Autoenconders
Hopfield Nets, Restricted Boltzmann Machines
Convolutional Nets, Deconvolutional Nets 
Generative Adversial Nets 
Deep Residuals Nets, Neural Turing Machines 
Transformers 
They all rely on modules 

# Training Deep Networks: Summary
---
1. Move input through the network to yield prediction 
2. Compare prediction to ground truth label 
3. Backpropagate errors to all weights 

# Chain Rule
---
$$
\frac{dz}{dx} = \frac{dz}{dy}|_{y(x)} \frac{dy}{dx}|_{x}
$$

For backpropagation, it is helpful to view function compositions as graphs. 
Each node in the graph indicates a variable, an operation is a simple function of one or more variables. 

# The chain rule beyond single inputs and outputs 
---

![[Screenshot 2024-11-03 at 21.19.44.png]]

# The Geometry of the Jacobian
---
The jacobian represents a local linearization of a function given a coordinate. 
Not unlike derivative being the the best linear approximation of a curve (tangent)

The jacobian determinant (for square matrices) measures the ratio of areas. 
Similiar to what the absolute slope measures in the 1d case (derivative)
![[Screenshot 2024-11-03 at 21.22.24.png]]

# Another Ingredient to Remember
---
![[Screenshot 2024-11-03 at 21.22.54.png]]

# Chain Rule of Composite Functions 
---
Make sure each component matches the dimensions. 
![[Screenshot 2024-11-03 at 21.24.03.png]]

# Backpropagation
---
The neural network loss is a composite function of modules.
We want the gradient wrt to the parametrs of the layer. 
Back Prop is an algorithm that computes the chain rule, with a specific order of operations that is highly efficient. 

# Autodiff
---
Forward: Compute the activation of each module in the network
Then, set $x_{l + 1} = h_l$
Store intermiadte variables for each module $h_l$: will be needed for the backprop and saves times at the cost of memory 
Then, repeat recursivly and in the right order. 


![[Screenshot 2024-11-03 at 21.27.46.png]]

# Backpropagation in One Image
---
![[Screenshot 2024-11-03 at 21.28.12.png]]

# Autodiff under the hood
---
Autodiff converts a program into a sequence of primitive operations. 
The operations have a pre-defined routines for computing derivatives. 
In this manner, backprop can be done mechanically. 

## Is just backprop? 
Another victim of Moravec's Paradox
Is all about bookkeeping, the rules as local 
Hence, a computer is much better equipped to deal with backpropagation.

## Makes Deep learning just like lego 

Deep learning libraries: repo with functions + pre-defined derivatives
Neural Networks: compositions of these functions 

You: Artists who combine these functions into architectures
Even if you make a new layer, no issue if you use existing parts 
Worst case: you only need to implement derivative of your new layer

