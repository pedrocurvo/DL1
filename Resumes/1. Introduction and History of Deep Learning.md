
# The Perceptron 

Single layer perceptron for binary classification
- One weight $w_i$ per input $x_i$
- Multiple each input with its weight, sum, and add bias 
- If result larger than threshold, return 1, otherwise 0

**Training the Perceptron** 
1. Set $w_j$ randomly
2. Sample new $(x_i, l_i)$
3. Compute $y_i = \sum_{i} w_i x_i > 0$
4. If $y_i < 0, l_i > 0 \rightarrow w_i = w_i + \eta \cdot x_i$ (Score too low, increase weights)
5. $y_i > 0, l_i < 0 \rightarrow w_i = w_i - \eta \cdot x_i$ (Score too high, decrease weights)
6. Go to 2

Problem: XOR is not linearly separable 

# Moravec's Paradox

Reasoning requires little computation, and perception from sensors a lot.

# Two Paths of Machine Learning Research 
## Path 1
Fix perceptron by making better features 

## Path 2
Fix perceptron by making them more complex 

---
## Path 1

**Philosophy:** Encode domain knowledge to help machine learning algorithms 
**Classical image recognition pipeline:**
1. Extract local features 
2. Aggregate local features over image 
3. Train classical models on aggregations 

# Path 2
Neural Networks beyond a single layer 

---
# Problems Associated with Deep Learning 
1. Deadly accidents 
2. Misinformation
3. Surveillance Concerns 
4. Bias 

